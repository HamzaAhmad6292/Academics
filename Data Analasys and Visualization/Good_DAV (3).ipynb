{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Data Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_wikipedia(language_code):\n",
        "    url = f\"https://{language_code}.wikipedia.org/wiki/Main_Page\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        articles = []\n",
        "        for heading in soup.find_all(\"p\", class_=\"\"):\n",
        "            articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "\n",
        "        return articles\n",
        "\n",
        "    except requests.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.RequestException as err:\n",
        "        print(f\"Other Error: {err}\")\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "languages = {\n",
        "    \"tt\": \"Tatar\",\n",
        "    \"en\": \"English\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"su\": \"Sundanese\",\n",
        "    \"he\": \"Hebrew\",\n",
        "    \"yo\": \"Yoruba\",\n",
        "    \"pa\": \"Punjabi\",\n",
        "}\n",
        "\n",
        "\n",
        "\n",
        "data = []\n",
        "for lang_code, lang_name in languages.items():\n",
        "    wiki_articles = scrape_wikipedia(lang_code)\n",
        "    if wiki_articles:\n",
        "        for article in wiki_articles:\n",
        "            data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "# for lang_code, lang_name in languages.items():\n",
        "#     wiki_articles = scrape_wikipedia(lang_code)\n",
        "#     if wiki_articles:\n",
        "#         for article in wiki_articles:\n",
        "#             data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.dropna(inplace=True)\n",
        "df.to_csv(\"multilingual_wikipedia_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "bCKBTzlhW79o"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Wrangling\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "\n",
        "\n",
        "# aggregated_df = df.groupby('Language')['Text'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "aggregated_df=pd.read_csv(\"multilingual_wikipedia_dataset.csv\")\n",
        "aggregated_df.dropna(inplace=True)\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].str.lower()\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "\n",
        "# aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [normalize('NFKD', word).encode('ASCII', 'ignore').decode('utf-8') for word in x])\n",
        "\n",
        "\n",
        "\n",
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAG_jYvv8RW4",
        "outputId": "7e470298-64c6-45b3-dc35-af1fa8a852de"
      },
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...\n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...\n",
            "6      Tatar  [аргентинада, велосипедчылар, кубада, төзүчелә...\n",
            "9      Tatar  [коену, рус, совет, һәм, америка, рәссамы, ник...\n",
            "10     Tatar  [николай, фешин, картинасы, җәйге, эсселек, ва...\n",
            "..       ...                                                ...\n",
            "335  Punjabi  [ਕ, ਗ, ਲ, ਕਤ, ਤਰ, ਗਣਰ, ਜ, ਦ, ਕ, ਮ, ਪ, ਰਕ, ਵ, ਖ...\n",
            "336  Punjabi  [ਉਪ, ਸ, ਰ, ਣ, ਆ, ਦ, ਖਣ, ਲਈ, ਕ, ਰਪ, ਕਰਕ, ਤ, ਰ, ...\n",
            "347  Punjabi  [ਇਹ, ਵ, ਕ, ਪ, ਡ, ਆ, ਪ, ਜ, ਬ, ਵ, ਚ, ਲ, ਖ, ਆ, ਗ,...\n",
            "348  Punjabi  [स, स, क, त, प, ल, भ, जप, र, मर, ठ, ಕನ, ನಡ, தம...\n",
            "350  Punjabi  [ਵ, ਕ, ਪ, ਡ, ਆ, ਵ, ਲ, ਟ, ਅਰ, ਸ, ਪ, ਦਕ, ਦ, ਆਰ, ...\n",
            "\n",
            "[255 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Embedding\n",
        "#\n",
        "row_counts = df['Language'].value_counts()\n",
        "print(row_counts.head(15))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmDJpjar__m3",
        "outputId": "dda43e80-ec75-4b6c-dc9d-3659f5e566c8"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tatar        79\n",
            "Italian      55\n",
            "Catalan      33\n",
            "Asturian     33\n",
            "Polish       33\n",
            "Arabic       32\n",
            "Sundanese    30\n",
            "Hebrew       28\n",
            "Yoruba       27\n",
            "Slovak       27\n",
            "Punjabi      27\n",
            "Name: Language, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.Language.unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WqcXfPi8Prd-",
        "outputId": "d3b03f4f-f135-4ff0-ce51-d4d1187f974a"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['Tatar', 'Italian', 'Catalan', 'Asturian', 'Polish', 'Arabic',\n",
              "       'Sundanese', 'Afrikaans', 'Hebrew', 'Yoruba', 'Slovak', 'Punjabi',\n",
              "       'Galician', 'Somali', 'Basque'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(aggregated_df['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word_embedding(sentence):\n",
        "    embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "aggregated_df['Word_Embedding'] = aggregated_df['Text'].apply(lambda x: average_word_embedding(x))\n",
        "\n",
        "df.to_csv(\"dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3II3nxJQAXvL"
      },
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nquTBcAXrE",
        "outputId": "1cab2fd0-c802-4cb2-bfb2-56dadd2e600d"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text  \\\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...   \n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...   \n",
            "6      Tatar  [аргентинада, велосипедчылар, кубада, төзүчелә...   \n",
            "9      Tatar  [коену, рус, совет, һәм, америка, рәссамы, ник...   \n",
            "10     Tatar  [николай, фешин, картинасы, җәйге, эсселек, ва...   \n",
            "..       ...                                                ...   \n",
            "449   Basque  [iparragirreren, gernikako, arbola, euskal, er...   \n",
            "451   Basque  [eaeko, auzitegi, nagusiak, udaletan, euskara,...   \n",
            "453   Basque  [urriaren, 7an, gaza, israel, gatazka, hasi, z...   \n",
            "454   Basque  [alan, griffin, denny, laine, concha, velasco,...   \n",
            "455   Basque  [alan, griffin, denny, laine, concha, velasco,...   \n",
            "\n",
            "                                        Word_Embedding  \n",
            "0    [0.00056917703, 0.00034485586, 0.00019627949, ...  \n",
            "1    [0.0030841334, 0.0003638751, 0.0011694995, -0....  \n",
            "6    [-0.0022239294, -0.00076946523, 0.0010692426, ...  \n",
            "9    [2.2264308e-05, -0.00014360264, -0.0018548886,...  \n",
            "10   [-0.0015709086, -0.0004128805, -0.0013188795, ...  \n",
            "..                                                 ...  \n",
            "449  [-0.0022958382, 0.0023000888, 0.0005659851, -0...  \n",
            "451  [0.0007305687, 9.6268974e-05, -0.0028691322, -...  \n",
            "453  [0.0022323786, 0.00035672492, -0.0008374095, 0...  \n",
            "454  [-0.0016185772, -0.00036031203, 0.002678162, -...  \n",
            "455  [-0.0016185772, -0.00036031203, 0.002678162, -...  \n",
            "\n",
            "[324 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'aggregated_df' is your dataset DataFrame\n",
        "# Assuming 'text' contains the text data for each sample\n",
        "# Assuming 'Language' is the target variable\n",
        "\n",
        "# Tokenize the text and pad sequences\n",
        "max_length = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aggregated_df['Text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "X = pad_sequences(tokenizer.texts_to_sequences(aggregated_df['Text']), maxlen=max_length, padding='post')\n",
        "# X = np.vstack(aggregated_df['Word_Embedding'])  # Assuming 'Word_Embedding' contains numpy arrays\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(aggregated_df['Language'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define and compile the model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=30, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9qhgl9ZBCQ",
        "outputId": "e2ba6c1f-8907-4f28-9428-8342b610c6c3"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "8/8 [==============================] - 9s 422ms/step - loss: 2.2935 - accuracy: 0.1616 - val_loss: 2.2538 - val_accuracy: 0.2692\n",
            "Epoch 2/30\n",
            "8/8 [==============================] - 2s 279ms/step - loss: 2.2470 - accuracy: 0.1659 - val_loss: 2.1635 - val_accuracy: 0.0769\n",
            "Epoch 3/30\n",
            "8/8 [==============================] - 2s 296ms/step - loss: 2.2449 - accuracy: 0.1747 - val_loss: 2.1487 - val_accuracy: 0.2308\n",
            "Epoch 4/30\n",
            "8/8 [==============================] - 4s 524ms/step - loss: 2.1858 - accuracy: 0.1834 - val_loss: 2.1091 - val_accuracy: 0.2692\n",
            "Epoch 5/30\n",
            "8/8 [==============================] - 3s 355ms/step - loss: 2.1972 - accuracy: 0.2096 - val_loss: 2.1000 - val_accuracy: 0.2692\n",
            "Epoch 6/30\n",
            "8/8 [==============================] - 2s 291ms/step - loss: 2.1555 - accuracy: 0.2620 - val_loss: 2.0653 - val_accuracy: 0.2308\n",
            "Epoch 7/30\n",
            "8/8 [==============================] - 2s 298ms/step - loss: 2.0643 - accuracy: 0.2795 - val_loss: 2.0120 - val_accuracy: 0.2692\n",
            "Epoch 8/30\n",
            "8/8 [==============================] - 2s 310ms/step - loss: 1.8796 - accuracy: 0.3624 - val_loss: 1.8359 - val_accuracy: 0.4615\n",
            "Epoch 9/30\n",
            "8/8 [==============================] - 4s 457ms/step - loss: 1.7472 - accuracy: 0.4236 - val_loss: 1.8291 - val_accuracy: 0.5769\n",
            "Epoch 10/30\n",
            "8/8 [==============================] - 4s 423ms/step - loss: 1.6865 - accuracy: 0.4934 - val_loss: 1.7275 - val_accuracy: 0.4615\n",
            "Epoch 11/30\n",
            "8/8 [==============================] - 2s 298ms/step - loss: 1.4394 - accuracy: 0.5939 - val_loss: 1.5648 - val_accuracy: 0.6923\n",
            "Epoch 12/30\n",
            "8/8 [==============================] - 2s 300ms/step - loss: 1.2411 - accuracy: 0.6332 - val_loss: 1.2353 - val_accuracy: 0.6923\n",
            "Epoch 13/30\n",
            "8/8 [==============================] - 2s 300ms/step - loss: 0.9612 - accuracy: 0.7205 - val_loss: 1.0178 - val_accuracy: 0.6538\n",
            "Epoch 14/30\n",
            "8/8 [==============================] - 3s 363ms/step - loss: 0.8527 - accuracy: 0.7336 - val_loss: 0.9429 - val_accuracy: 0.6923\n",
            "Epoch 15/30\n",
            "8/8 [==============================] - 4s 525ms/step - loss: 0.6914 - accuracy: 0.7948 - val_loss: 0.6128 - val_accuracy: 0.7692\n",
            "Epoch 16/30\n",
            "8/8 [==============================] - 2s 293ms/step - loss: 0.6052 - accuracy: 0.8210 - val_loss: 0.5868 - val_accuracy: 0.8077\n",
            "Epoch 17/30\n",
            "8/8 [==============================] - 2s 288ms/step - loss: 0.4734 - accuracy: 0.8690 - val_loss: 0.6433 - val_accuracy: 0.8077\n",
            "Epoch 18/30\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 0.3745 - accuracy: 0.9083 - val_loss: 0.5387 - val_accuracy: 0.8077\n",
            "Epoch 19/30\n",
            "8/8 [==============================] - 2s 297ms/step - loss: 0.3396 - accuracy: 0.9258 - val_loss: 0.4054 - val_accuracy: 0.8846\n",
            "Epoch 20/30\n",
            "8/8 [==============================] - 4s 565ms/step - loss: 0.3667 - accuracy: 0.8821 - val_loss: 0.5224 - val_accuracy: 0.8462\n",
            "Epoch 21/30\n",
            "8/8 [==============================] - 3s 312ms/step - loss: 0.2604 - accuracy: 0.9345 - val_loss: 0.4682 - val_accuracy: 0.8077\n",
            "Epoch 22/30\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 0.2506 - accuracy: 0.9389 - val_loss: 0.4822 - val_accuracy: 0.8077\n",
            "Epoch 23/30\n",
            "8/8 [==============================] - 2s 288ms/step - loss: 0.2139 - accuracy: 0.9432 - val_loss: 0.3921 - val_accuracy: 0.8462\n",
            "Epoch 24/30\n",
            "8/8 [==============================] - 2s 290ms/step - loss: 0.1864 - accuracy: 0.9694 - val_loss: 0.3612 - val_accuracy: 0.8462\n",
            "Epoch 25/30\n",
            "8/8 [==============================] - 4s 468ms/step - loss: 0.1680 - accuracy: 0.9825 - val_loss: 0.3905 - val_accuracy: 0.8462\n",
            "Epoch 26/30\n",
            "8/8 [==============================] - 4s 426ms/step - loss: 0.1620 - accuracy: 0.9520 - val_loss: 0.4293 - val_accuracy: 0.8462\n",
            "Epoch 27/30\n",
            "8/8 [==============================] - 2s 293ms/step - loss: 0.1340 - accuracy: 0.9738 - val_loss: 0.4774 - val_accuracy: 0.8462\n",
            "Epoch 28/30\n",
            "8/8 [==============================] - 3s 317ms/step - loss: 0.1100 - accuracy: 0.9825 - val_loss: 0.4520 - val_accuracy: 0.8462\n",
            "Epoch 29/30\n",
            "8/8 [==============================] - 2s 285ms/step - loss: 0.1162 - accuracy: 0.9694 - val_loss: 0.5337 - val_accuracy: 0.8077\n",
            "Epoch 30/30\n",
            "8/8 [==============================] - 3s 357ms/step - loss: 0.0956 - accuracy: 0.9782 - val_loss: 0.6921 - val_accuracy: 0.7692\n",
            "1/1 [==============================] - 0s 103ms/step - loss: 0.6921 - accuracy: 0.7692\n",
            "Test Accuracy: 0.7692307829856873\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRQM6FNpckut",
        "outputId": "00e332de-6aba-4ba9-ef9d-085b8ee58a43"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "    Language                                               Text  \\\n",
            "0      Tatar  [ирекле, эчтәлекле, энциклопедияне, һәркем, яз...   \n",
            "1      Tatar  [тулы, исемлек, эчтәлек, порталлар, latin, iml...   \n",
            "6      Tatar  [аргентинада, велосипедчылар, кубада, төзүчелә...   \n",
            "9      Tatar  [коену, рус, совет, һәм, америка, рәссамы, ник...   \n",
            "10     Tatar  [николай, фешин, картинасы, җәйге, эсселек, ва...   \n",
            "..       ...                                                ...   \n",
            "335  Punjabi  [ਕ, ਗ, ਲ, ਕਤ, ਤਰ, ਗਣਰ, ਜ, ਦ, ਕ, ਮ, ਪ, ਰਕ, ਵ, ਖ...   \n",
            "336  Punjabi  [ਉਪ, ਸ, ਰ, ਣ, ਆ, ਦ, ਖਣ, ਲਈ, ਕ, ਰਪ, ਕਰਕ, ਤ, ਰ, ...   \n",
            "347  Punjabi  [ਇਹ, ਵ, ਕ, ਪ, ਡ, ਆ, ਪ, ਜ, ਬ, ਵ, ਚ, ਲ, ਖ, ਆ, ਗ,...   \n",
            "348  Punjabi  [स, स, क, त, प, ल, भ, जप, र, मर, ठ, ಕನ, ನಡ, தம...   \n",
            "350  Punjabi  [ਵ, ਕ, ਪ, ਡ, ਆ, ਵ, ਲ, ਟ, ਅਰ, ਸ, ਪ, ਦਕ, ਦ, ਆਰ, ...   \n",
            "\n",
            "                                        Word_Embedding  \n",
            "0    [-0.0016337071, -0.00082817115, -0.001665919, ...  \n",
            "1    [-0.0016191887, -0.00067272974, 0.0011436198, ...  \n",
            "6    [0.0001273412, 0.00037375442, -0.0030039856, 0...  \n",
            "9    [0.0002474672, -0.00088088954, 0.00091339106, ...  \n",
            "10   [-0.00083082065, -0.0003023588, -0.0004749676,...  \n",
            "..                                                 ...  \n",
            "335  [-0.003288348, 0.005151944, -0.00039769543, 0....  \n",
            "336  [-0.0006203922, 0.004632704, 0.001261495, -7.3...  \n",
            "347  [-0.0034989277, 0.002807998, 0.00084127, 0.000...  \n",
            "348  [-0.0003962175, -0.0014884339, 0.0010652036, -...  \n",
            "350  [-0.004762963, 0.0038338043, 0.0013435486, 0.0...  \n",
            "\n",
            "[255 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Generate predictions on test data\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)  # Get the predicted classes\n",
        "\n",
        "# Convert encoded labels back to original labelsma\n",
        "y_test_original = label_encoder.inverse_transform(y_test)\n",
        "y_pred_original = label_encoder.inverse_transform(y_pred_classes)\n",
        "\n",
        "# Generate classification report\n",
        "print(classification_report(y_test_original, y_pred_original))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOqDhhkdyUq",
        "outputId": "3a48c480-7d82-433f-bf7a-10914aefe151"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 111ms/step\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "      Arabic       1.00      1.00      1.00         2\n",
            "     Catalan       1.00      1.00      1.00         1\n",
            "     English       0.00      0.00      0.00         0\n",
            "      Hebrew       0.60      1.00      0.75         3\n",
            "     Italian       1.00      1.00      1.00         2\n",
            "      Polish       1.00      0.33      0.50         3\n",
            "   Sundanese       0.57      1.00      0.73         4\n",
            "       Tatar       1.00      0.43      0.60         7\n",
            "      Yoruba       1.00      1.00      1.00         4\n",
            "\n",
            "    accuracy                           0.77        26\n",
            "   macro avg       0.80      0.75      0.73        26\n",
            "weighted avg       0.89      0.77      0.76        26\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Assuming 'text_to_predict' is the text you want to predict the language for\n",
        "text_to_predict = \"улы исемлек эчтәлек порталлар latin\"\n",
        "\n",
        "# Preprocess the text\n",
        "sequence = tokenizer.texts_to_sequences([text_to_predict])\n",
        "padded_sequence = pad_sequences(sequence, maxlen=max_length, padding='post')\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(padded_sequence)\n",
        "predicted_language_index = np.argmax(predictions)\n",
        "predicted_language = label_encoder.classes_[predicted_language_index]\n",
        "\n",
        "# Print the predicted language\n",
        "print(\"Predicted Language:\", predicted_language)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vTPV9CQzxx43",
        "outputId": "9b146c0c-3eb9-4abd-df12-30de16fb158a"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 102ms/step\n",
            "Predicted Language: Tatar\n"
          ]
        }
      ]
    }
  ]
}