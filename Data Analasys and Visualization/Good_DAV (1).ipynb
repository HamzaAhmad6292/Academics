{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Data Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_wikipedia(language_code):\n",
        "    url = f\"https://{language_code}.wikipedia.org/wiki/Main_Page\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        articles = []\n",
        "        for heading in soup.find_all(\"p\", class_=\"\"):\n",
        "            articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "\n",
        "        return articles\n",
        "\n",
        "    except requests.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.RequestException as err:\n",
        "        print(f\"Other Error: {err}\")\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "languages = {\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"fr\": \"French\",\n",
        "    \"de\": \"German\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"ja\": \"Japanese\",\n",
        "    \"zh\": \"Chinese\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"ko\": \"Korean\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"he\": \"Hebrew\",\n",
        "    \"th\": \"Thai\",\n",
        "    \"da\": \"Danish\",\n",
        "    \"fi\": \"Finnish\",\n",
        "    \"id\": \"Indonesian\",\n",
        "    \"no\": \"Norwegian\",\n",
        "    \"hu\": \"Hungarian\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"sk\": \"Slovak\",\n",
        "    \"vi\": \"Vietnamese\",\n",
        "    \"bn\": \"Bengali\",\n",
        "    \"sr\": \"Serbian\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"ms\": \"Malay\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"gl\": \"Galician\",\n",
        "    \"ta\": \"Tamil\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"ur\": \"Urdu\",\n",
        "    \"eu\": \"Basque\",\n",
        "    \"be\": \"Belarusian\",\n",
        "    \"is\": \"Icelandic\",\n",
        "    \"af\": \"Afrikaans\",\n",
        "    \"sw\": \"Swahili\",\n",
        "    \"am\": \"Amharic\",\n",
        "    \"fa\": \"Persian\",\n",
        "    \"ne\": \"Nepali\",\n",
        "    \"so\": \"Somali\",\n",
        "    \"ka\": \"Georgian\",\n",
        "    \"la\": \"Latin\",\n",
        "    \"km\": \"Khmer\",\n",
        "    \"mk\": \"Macedonian\",\n",
        "    \"mn\": \"Mongolian\",\n",
        "    \"fy\": \"Western Frisian\",\n",
        "    \"gu\": \"Gujarati\",\n",
        "    \"pa\": \"Punjabi\",\n",
        "    \"jv\": \"Javanese\",\n",
        "    \"ceb\": \"Cebuano\",\n",
        "    \"ha\": \"Hausa\",\n",
        "    \"yo\": \"Yoruba\",\n",
        "    \"su\": \"Sundanese\",\n",
        "    \"si\": \"Sinhala\",\n",
        "    \"sn\": \"Shona\",\n",
        "    \"hmn\": \"Hmong\",\n",
        "    \"uz\": \"Uzbek\",\n",
        "    \"rw\": \"Kinyarwanda\",\n",
        "    \"glg\": \"Galician\",\n",
        "    \"kk\": \"Kazakh\",\n",
        "    \"ku\": \"Kurdish\",\n",
        "    \"tt\": \"Tatar\",\n",
        "    \"ast\": \"Asturian\",\n",
        "\n",
        "}\n",
        "\n",
        "\n",
        "data = []\n",
        "for lang_code, lang_name in languages.items():\n",
        "    wiki_articles = scrape_wikipedia(lang_code)\n",
        "    if wiki_articles:\n",
        "        for article in wiki_articles:\n",
        "            data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "# for lang_code, lang_name in languages.items():\n",
        "#     wiki_articles = scrape_wikipedia(lang_code)\n",
        "#     if wiki_articles:\n",
        "#         for article in wiki_articles:\n",
        "#             data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"multilingual_wikipedia_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCKBTzlhW79o",
        "outputId": "67b69245-ed25-4060-aaa0-9f59d420bc9b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Connecting: HTTPSConnectionPool(host='hmn.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Main_Page (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b47813c1930>: Failed to resolve 'hmn.wikipedia.org' ([Errno -2] Name or service not known)\"))\n",
            "Error Connecting: HTTPSConnectionPool(host='glg.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Main_Page (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7b478148a350>: Failed to resolve 'glg.wikipedia.org' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Wrangling\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "\n",
        "\n",
        "# aggregated_df = df.groupby('Language')['Text'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "aggregated_df=df\n",
        "aggregated_df.dropna(inplace=True)\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].str.lower()\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [normalize('NFKD', word).encode('ASCII', 'ignore').decode('utf-8') for word in x])\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HAG_jYvv8RW4",
        "outputId": "f1994892-cd72-435e-cb8a-9b20b196415d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      Language                                               Text\n",
            "0      English  [janet, s, is, the, tenth, episode, of, the, t...\n",
            "1      English  [december, 6, saint, nicholas, day, western, c...\n",
            "2      English  [keke, rosberg, born, 6, december, 1948, is, a...\n",
            "3      English  [photograph, credit, hans, van, dijk, restored...\n",
            "4      English  [wikipedia, is, written, by, volunteer, editor...\n",
            "...        ...                                                ...\n",
            "1167  Asturian                                                 []\n",
            "1168  Asturian        [ver, la, llista, completa, de, wikipedies]\n",
            "1169  Asturian                [proyeutos, hermanos, n, asturianu]\n",
            "1170  Asturian                                                 []\n",
            "1171  Asturian                                                 []\n",
            "\n",
            "[1172 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Embedding\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmDJpjar__m3",
        "outputId": "6e34a2cf-c184-46c1-beea-75deb3d0e86a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.4)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(aggregated_df['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word_embedding(sentence):\n",
        "    embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "aggregated_df['Word_Embedding'] = aggregated_df['Text'].apply(lambda x: average_word_embedding(x))\n",
        "\n",
        "df.to_csv(\"dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3II3nxJQAXvL"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nquTBcAXrE",
        "outputId": "295e655b-7177-49d6-b1b7-7fb1fbc7534c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Language                                               Text  \\\n",
            "0       English  [hurricane, erika, was, a, weak, hurricane, th...   \n",
            "1       English  [december, 5, krampusnacht, in, parts, of, cen...   \n",
            "2       English  [columbidae, is, a, bird, family, consisting, ...   \n",
            "3       English            [photograph, credit, charles, j, sharp]   \n",
            "4       English  [wikipedia, is, written, by, volunteer, editor...   \n",
            "...         ...                                                ...   \n",
            "1260    Latvian  [vikipedija, ir, bezpelnas, organizacijas, wik...   \n",
            "1261  Bulgarian                                                 []   \n",
            "1262  Bulgarian                                                 []   \n",
            "1263  Bulgarian                                                 []   \n",
            "1264  Bulgarian                                                 []   \n",
            "\n",
            "                                         Word_Embedding  \n",
            "0     [0.0005811478, 0.00051339774, 0.0009449449, 0....  \n",
            "1     [0.0011380664, -0.0002042058, 0.00096702675, 0...  \n",
            "2     [0.00048020238, 0.00076636963, 0.0010078207, 0...  \n",
            "3     [-0.0023510922, -0.00045195833, 0.0024440165, ...  \n",
            "4     [-0.000533366, 0.0031791816, 0.0016209107, 0.0...  \n",
            "...                                                 ...  \n",
            "1260  [0.0011170198, 0.0019475785, 0.0015682884, -0....  \n",
            "1261  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1262  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1263  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1264  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "\n",
            "[1265 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'aggregated_df' is your dataset DataFrame\n",
        "# Assuming 'text' contains the text data for each sample\n",
        "# Assuming 'Language' is the target variable\n",
        "\n",
        "# Tokenize the text and pad sequences\n",
        "max_length = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aggregated_df['Text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# X = pad_sequences(tokenizer.texts_to_sequences(aggregated_df['Text']), maxlen=max_length, padding='post')\n",
        "X = np.vstack(aggregated_df['Word_Embedding'])  # Assuming 'Word_Embedding' contains numpy arrays\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(aggregated_df['Language'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define and compile the model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9qhgl9ZBCQ",
        "outputId": "04838fdf-5b47-4070-9a4f-de3e8784b77d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "8/8 [==============================] - 6s 291ms/step - loss: 2.4469 - accuracy: 0.2381 - val_loss: 2.3780 - val_accuracy: 0.2857\n",
            "Epoch 2/2\n",
            "8/8 [==============================] - 2s 205ms/step - loss: 2.3446 - accuracy: 0.2857 - val_loss: 2.3024 - val_accuracy: 0.2857\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 2.3024 - accuracy: 0.2857\n",
            "Test Accuracy: 0.2857142984867096\n"
          ]
        }
      ]
    }
  ]
}