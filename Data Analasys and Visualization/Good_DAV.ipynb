{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Data Scraping\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_wikipedia(language_code):\n",
        "    url = f\"https://{language_code}.wikipedia.org/wiki/Main_Page\"\n",
        "    headers = {\n",
        "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/94.0.4606.81 Safari/537.36\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, headers=headers)\n",
        "        response.raise_for_status()\n",
        "        soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        articles = []\n",
        "        for heading in soup.find_all(\"p\", class_=\"\"):\n",
        "            articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "        # response = requests.get(url, headers=headers)\n",
        "        # response.raise_for_status()\n",
        "        # soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "        # for heading in soup.find_all(\"span\", class_=\"mw-headline\"):\n",
        "        #     articles.append(heading.text.strip())\n",
        "\n",
        "\n",
        "        return articles\n",
        "\n",
        "    except requests.HTTPError as errh:\n",
        "        print(f\"HTTP Error: {errh}\")\n",
        "    except requests.ConnectionError as errc:\n",
        "        print(f\"Error Connecting: {errc}\")\n",
        "    except requests.Timeout as errt:\n",
        "        print(f\"Timeout Error: {errt}\")\n",
        "    except requests.RequestException as err:\n",
        "        print(f\"Other Error: {err}\")\n",
        "    return []\n",
        "\n",
        "\n",
        "\n",
        "languages = {\n",
        "    \"en\": \"English\",\n",
        "    \"es\": \"Spanish\",\n",
        "    \"fr\": \"French\",\n",
        "    \"de\": \"German\",\n",
        "    \"it\": \"Italian\",\n",
        "    \"nl\": \"Dutch\",\n",
        "    \"pt\": \"Portuguese\",\n",
        "    \"ru\": \"Russian\",\n",
        "    \"ja\": \"Japanese\",\n",
        "    \"zh\": \"Chinese\",\n",
        "    \"ar\": \"Arabic\",\n",
        "    \"hi\": \"Hindi\",\n",
        "    \"ko\": \"Korean\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"pl\": \"Polish\",\n",
        "    \"sv\": \"Swedish\",\n",
        "    \"uk\": \"Ukrainian\",\n",
        "    \"cs\": \"Czech\",\n",
        "    \"el\": \"Greek\",\n",
        "    \"he\": \"Hebrew\",\n",
        "    \"th\": \"Thai\",\n",
        "    \"da\": \"Danish\",\n",
        "    \"fi\": \"Finnish\",\n",
        "    \"id\": \"Indonesian\",\n",
        "    \"no\": \"Norwegian\",\n",
        "    \"hu\": \"Hungarian\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"sk\": \"Slovak\",\n",
        "    \"vi\": \"Vietnamese\",\n",
        "    \"bn\": \"Bengali\",\n",
        "    \"sr\": \"Serbian\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"ms\": \"Malay\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"gl\": \"Galician\",\n",
        "    \"ta\": \"Tamil\",\n",
        "    \"ca\": \"Catalan\",\n",
        "    \"ur\": \"Urdu\",\n",
        "    \"eu\": \"Basque\",\n",
        "    \"be\": \"Belarusian\",\n",
        "    \"is\": \"Icelandic\",\n",
        "    \"af\": \"Afrikaans\",\n",
        "    \"sw\": \"Swahili\",\n",
        "    \"am\": \"Amharic\",\n",
        "    \"fa\": \"Persian\",\n",
        "    \"ne\": \"Nepali\",\n",
        "    \"so\": \"Somali\",\n",
        "    \"ka\": \"Georgian\",\n",
        "    \"la\": \"Latin\",\n",
        "    \"km\": \"Khmer\",\n",
        "    \"mk\": \"Macedonian\",\n",
        "    \"mn\": \"Mongolian\",\n",
        "    \"fy\": \"Western Frisian\",\n",
        "    \"gu\": \"Gujarati\",\n",
        "    \"pa\": \"Punjabi\",\n",
        "    \"jv\": \"Javanese\",\n",
        "    \"ceb\": \"Cebuano\",\n",
        "    \"ha\": \"Hausa\",\n",
        "    \"yo\": \"Yoruba\",\n",
        "    \"su\": \"Sundanese\",\n",
        "    \"si\": \"Sinhala\",\n",
        "    \"sn\": \"Shona\",\n",
        "    \"hmn\": \"Hmong\",\n",
        "    \"uz\": \"Uzbek\",\n",
        "    \"rw\": \"Kinyarwanda\",\n",
        "    \"glg\": \"Galician\",\n",
        "    \"kk\": \"Kazakh\",\n",
        "    \"ku\": \"Kurdish\",\n",
        "    \"tt\": \"Tatar\",\n",
        "    \"ast\": \"Asturian\",\n",
        "    \"qu\": \"Quechua\",\n",
        "    \"tl\": \"Tagalog\",\n",
        "    \"tg\": \"Tajik\",\n",
        "    \"my\": \"Burmese\",\n",
        "    \"or\": \"Odia\",\n",
        "    \"ky\": \"Kyrgyz\",\n",
        "    \"et\": \"Estonian\",\n",
        "    \"lt\": \"Lithuanian\",\n",
        "    \"lv\": \"Latvian\",\n",
        "    \"fi\": \"Finnish\",\n",
        "    \"hr\": \"Croatian\",\n",
        "    \"sr\": \"Serbian\",\n",
        "    \"sk\": \"Slovak\",\n",
        "    \"sl\": \"Slovenian\",\n",
        "    \"ms\": \"Malay\",\n",
        "    \"th\": \"Thai\",\n",
        "    \"vi\": \"Vietnamese\",\n",
        "    \"bg\": \"Bulgarian\",\n",
        "    \"ro\": \"Romanian\",\n",
        "    \"tr\": \"Turkish\",\n",
        "    \"is\": \"Icelandic\",\n",
        "    \"hu\": \"Hungarian\",\n",
        "}\n",
        "\n",
        "\n",
        "data = []\n",
        "for lang_code, lang_name in languages.items():\n",
        "    wiki_articles = scrape_wikipedia(lang_code)\n",
        "    if wiki_articles:\n",
        "        for article in wiki_articles:\n",
        "            data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "# for lang_code, lang_name in languages.items():\n",
        "#     wiki_articles = scrape_wikipedia(lang_code)\n",
        "#     if wiki_articles:\n",
        "#         for article in wiki_articles:\n",
        "#             data.append({\"Language\": lang_name, \"Text\": article})\n",
        "\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "df.to_csv(\"multilingual_wikipedia_dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bCKBTzlhW79o",
        "outputId": "f6777575-7f70-4da9-f1c1-fc745acc1425"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error Connecting: HTTPSConnectionPool(host='hmn.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Main_Page (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bc84cbf4d90>: Failed to resolve 'hmn.wikipedia.org' ([Errno -2] Name or service not known)\"))\n",
            "Error Connecting: HTTPSConnectionPool(host='glg.wikipedia.org', port=443): Max retries exceeded with url: /wiki/Main_Page (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x7bc84ca41360>: Failed to resolve 'glg.wikipedia.org' ([Errno -2] Name or service not known)\"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Data Wrangling\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import re\n",
        "from unicodedata import normalize\n",
        "\n",
        "\n",
        "\n",
        "# aggregated_df = df.groupby('Language')['Text'].agg(lambda x: ' '.join(x)).reset_index()\n",
        "aggregated_df=df\n",
        "aggregated_df.fillna(inplace=True)\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].str.lower()\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: re.findall(r'\\b\\w+\\b', x))\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [normalize('NFKD', word).encode('ASCII', 'ignore').decode('utf-8') for word in x])\n",
        "\n",
        "\n",
        "aggregated_df['Text'] = aggregated_df['Text'].apply(lambda x: [word for word in x if word.isalnum()])\n",
        "\n",
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "id": "HAG_jYvv8RW4",
        "outputId": "774f172d-384d-4241-93cc-f699d128745a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-0866444e1613>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# aggregated_df = df.groupby('Language')['Text'].agg(lambda x: ' '.join(x)).reset_index()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0maggregated_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0maggregated_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0maggregated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maggregated_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   5633\u001b[0m         \u001b[0mdowncast\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5634\u001b[0m     ) -> DataFrame | None:\n\u001b[0;32m-> 5635\u001b[0;31m         return super().fillna(\n\u001b[0m\u001b[1;32m   5636\u001b[0m             \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5637\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mfillna\u001b[0;34m(self, value, method, axis, inplace, limit, downcast)\u001b[0m\n\u001b[1;32m   6793\u001b[0m         \"\"\"\n\u001b[1;32m   6794\u001b[0m         \u001b[0minplace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_bool_kwarg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"inplace\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6795\u001b[0;31m         \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_fillna_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6797\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_consolidate_inplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_validators.py\u001b[0m in \u001b[0;36mvalidate_fillna_kwargs\u001b[0;34m(value, method, validate_scalar_dict_value)\u001b[0m\n\u001b[1;32m    386\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Must specify a fill 'value' or 'method'.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmethod\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m         \u001b[0mmethod\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_fill_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Must specify a fill 'value' or 'method'."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Embedding\n",
        "!pip install gensim\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QmDJpjar__m3",
        "outputId": "525add30-0aa0-4c7d-9309-91eb50f004ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.11.3)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (6.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "word2vec_model = Word2Vec(aggregated_df['Text'], vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "def average_word_embedding(sentence):\n",
        "    embeddings = [word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
        "    if embeddings:\n",
        "        return np.mean(embeddings, axis=0)\n",
        "    else:\n",
        "        return np.zeros(word2vec_model.vector_size)\n",
        "\n",
        "aggregated_df['Word_Embedding'] = aggregated_df['Text'].apply(lambda x: average_word_embedding(x))\n",
        "\n",
        "df.to_csv(\"dataset.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "3II3nxJQAXvL"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(aggregated_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nquTBcAXrE",
        "outputId": "295e655b-7177-49d6-b1b7-7fb1fbc7534c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "       Language                                               Text  \\\n",
            "0       English  [hurricane, erika, was, a, weak, hurricane, th...   \n",
            "1       English  [december, 5, krampusnacht, in, parts, of, cen...   \n",
            "2       English  [columbidae, is, a, bird, family, consisting, ...   \n",
            "3       English            [photograph, credit, charles, j, sharp]   \n",
            "4       English  [wikipedia, is, written, by, volunteer, editor...   \n",
            "...         ...                                                ...   \n",
            "1260    Latvian  [vikipedija, ir, bezpelnas, organizacijas, wik...   \n",
            "1261  Bulgarian                                                 []   \n",
            "1262  Bulgarian                                                 []   \n",
            "1263  Bulgarian                                                 []   \n",
            "1264  Bulgarian                                                 []   \n",
            "\n",
            "                                         Word_Embedding  \n",
            "0     [0.0005811478, 0.00051339774, 0.0009449449, 0....  \n",
            "1     [0.0011380664, -0.0002042058, 0.00096702675, 0...  \n",
            "2     [0.00048020238, 0.00076636963, 0.0010078207, 0...  \n",
            "3     [-0.0023510922, -0.00045195833, 0.0024440165, ...  \n",
            "4     [-0.000533366, 0.0031791816, 0.0016209107, 0.0...  \n",
            "...                                                 ...  \n",
            "1260  [0.0011170198, 0.0019475785, 0.0015682884, -0....  \n",
            "1261  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1262  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1263  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "1264  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...  \n",
            "\n",
            "[1265 rows x 3 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, GlobalMaxPooling1D, Dense, Bidirectional, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming 'aggregated_df' is your dataset DataFrame\n",
        "# Assuming 'text' contains the text data for each sample\n",
        "# Assuming 'Language' is the target variable\n",
        "\n",
        "# Tokenize the text and pad sequences\n",
        "max_length = 100\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(aggregated_df['Text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "# X = pad_sequences(tokenizer.texts_to_sequences(aggregated_df['Text']), maxlen=max_length, padding='post')\n",
        "X = np.vstack(aggregated_df['Word_Embedding'])  # Assuming 'Word_Embedding' contains numpy arrays\n",
        "# Encode the target variable\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(aggregated_df['Language'])\n",
        "\n",
        "# Split the data into training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
        "\n",
        "# Define and compile the model\n",
        "embedding_dim = 100\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=max_length, trainable=True),\n",
        "    Bidirectional(LSTM(128, return_sequences=True)),\n",
        "    GlobalMaxPooling1D(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "\n",
        "    Dense(len(label_encoder.classes_), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, epochs=2, batch_size=32, validation_data=(X_test, y_test))\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MV9qhgl9ZBCQ",
        "outputId": "7479ad87-637f-436e-ac87-6d87558a8e6a"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "36/36 [==============================] - 20s 407ms/step - loss: 4.3419 - accuracy: 0.0220 - val_loss: 4.2717 - val_accuracy: 0.0866\n",
            "Epoch 2/2\n",
            "36/36 [==============================] - 14s 389ms/step - loss: 4.2875 - accuracy: 0.0413 - val_loss: 4.1892 - val_accuracy: 0.0866\n",
            "4/4 [==============================] - 0s 87ms/step - loss: 4.1892 - accuracy: 0.0866\n",
            "Test Accuracy: 0.08661417663097382\n"
          ]
        }
      ]
    }
  ]
}